\section{Related Work}

The pursuit of recursive self-improvement has a long history in AI. Schmidhuber’s formulation of Gödel Machines \cite{schmidhuber2007godel} introduced the notion of agents capable of provably optimal self-modification, laying the theoretical foundation for self-referential intelligence. Recent advances have translated these ideas into practical language agent architectures. Most notably, the Gödel Agent framework \cite{yin2024g} demonstrates how large language models (LLMs) can engage in self-referential reasoning to repair and enhance their own policies. While these developments provide an important proof of concept, they remain largely tied to frontier-scale LLMs, leaving open the question of whether smaller models can sustain recursive improvement under resource constraints. Our work addresses this gap by extending self-referential frameworks to smaller language models (SLMs) such as Qwen2.5-7B, and by proposing a principled mechanism for experience abstraction and policy repair that enables continual self-improvement.

Research on reflection-driven language agents provides another strand of inspiration. Approaches such as ReAct \cite{yao2023react}, Reflexion \cite{shinn2023reflexion}, and Self-Refine \cite{madaan2023self} show that iterative feedback and self-critique can substantially improve reasoning and task performance. Extensions like CRITIC \cite{gou2023critic} and self-debugging strategies \cite{chen2023teaching} further emphasize the value of embedding correction mechanisms into the agent loop. These works, however, primarily focus on improving responses within tasks. Polaris builds on their insight but moves beyond single-instance correction by abstracting from task-level failures, synthesizing reusable strategies, and integrating them back into the policy to enable cumulative improvement.

A complementary line of work investigates direct editing of model representations. Techniques for localizing and modifying factual associations \cite{meng2022locating}, performing mass edits in transformers \cite{meng2023mass}, and leveraging task arithmetic \cite{ilharco2022editing} illustrate that targeted modifications can shift model behavior without retraining. Surveys such as Wang et al. \cite{wang2024knowledge} summarize this rapidly expanding literature. Compared to such parameter-centric approaches, Polaris adopts a higher-level repair process, emphasizing strategy abstraction and policy refinement rather than surgical edits to weights.

The idea of repair also resonates with research in automated program repair (APR), where systems diagnose errors and generate patches to improve external code bases. Classical bibliographies \cite{monperrus2018automatic} and recent surveys on LLM-driven APR \cite{zhang2024systematic} reveal striking parallels to self-improvement in language agents. Our contribution differs in that repair is applied not to external programs but to the agent’s own evolving policy, thereby blurring the line between debugging and learning.

Finally, our work is informed by the tradition of open-ended learning, which treats novelty, diversity, and complexity as engines of continual progress. Theories of novelty search \cite{lehman2011abandoning,stanley2015greatness} and systems like POET \cite{wang2019paired} illustrate how adaptive agents coupled with evolving environments can produce unbounded improvement. Meta-learning \cite{finn2017model} and hierarchical reinforcement learning \cite{sutton1999between,bacon2017option} further demonstrate the importance of abstraction and reuse in sustaining adaptability. Polaris integrates these principles by framing recursive self-improvement not as a one-off adaptation but as an open-ended process in which failures yield reusable strategies that refine future behavior.
